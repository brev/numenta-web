<!DOCTYPE html>
 <html lang="en"><head><title data-react-helmet="true">Learning Through Active Exploration | Numenta.com</title><meta data-react-helmet="true" charset="utf-8"/><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1.0"/><meta data-react-helmet="true" http-equiv="X-UA-Compatible" content="IE=edge"/><meta data-react-helmet="true" name="author" content="Numenta"/><meta data-react-helmet="true" name="description" content="Numenta — Leading the New Era of Machine Intelligence"/><meta data-react-helmet="true" name="generator" content="© Numenta.com v0.2.25 Wed Jul 27 2016 11:34:50 GMT-0700 (PDT) Gatsby.js"/><meta data-react-helmet="true" name="keywords" content="@TODO,TODO,TODO"/><link data-react-helmet="true" rel="shortcut icon" href="/numenta-web/assets/icons/favicon.ico?v=0.2.25"/><link data-react-helmet="true" rel="stylesheet" href="/numenta-web/styles.css?v=0.2.25"/></head><body class="body"><div id="react-mount"><div class="style_layout_29A tachyons_f4_1Zt tachyons_f5-ns_QvA tachyons_fw4_3tj" data-reactroot="" data-reactid="1" data-react-checksum="-1277651086"><div class="style_appbar_3gV tachyons_bg-white_WF0 tachyons_fixed_I-c tachyons_h3_bmk tachyons_ph3_Qxl tachyons_shadow-1_8y5 tachyons_top-0_2pS tachyons_w-100_1xC" data-reactid="2"><header class="style_header_21a tachyons_center_iwf tachyons_cf_g7R tachyons_mw-100_ge3 tachyons_mw7-m_FjT tachyons_mw8-l_KcP" data-reactid="3"><div class="style_logo_t1Q tachyons_fl_wJK tachyons_relative_1fO tachyons_top-1_UVW" data-reactid="4"><a class="style_imagelink_kuG tachyons_dim_1n1 tachyons_link_2zV theme_color-blue_2lo style__visited_3qZ" href="/numenta-web/" data-reactid="5"><img alt="Numenta Logo" class="style_image_Kv4 tachyons_br2_rOa" src="/numenta-web/78d7c43a01aa37f432db4cb375ee2b7f.svg" title="Numenta" data-reactid="6"/></a></div><div class="style_search_Gt9 tachyons_fr_2Nn tachyons_mt3_11A" data-reactid="7"><form accept-charset="UTF-8" autocomplete="off" class="style_form_BAR tachyons_pa0_RbR tachyons_mb3_1mV tachyons_mh0_2XV tachyons_mt0_12u" enctype="multipart/form-data" method="post" data-reactid="8"><span class="style_label_2rM tachyons_clip_2YG" data-reactid="9"><label class="style_formLabel_13N tachyons_f5_2mB tachyons_fl_wJK tachyons_fw6_1NV tachyons_mr3_1Ez tachyons_relative_1fO tachyons_v-top_1JD tachyons_w4_3jJ" for="q" data-reactid="10">Search</label></span><span class="style_query_1Mw tachyons_fl_wJK tachyons_flex_22H tachyons_mb2_1U4 tachyons_relative_1fO tachyons_w4_3jJ tachyons_w5-ns_1tF" data-reactid="11"><input type="search" class="style_formInput_oLS tachyons_ba_1NL tachyons_b--black-20_1zc tachyons_br2_rOa tachyons_f5_2mB tachyons_flex-auto_1Fv tachyons_fw3_3xI tachyons_input-reset_3Az tachyons_pb1_2R8 tachyons_ph2_2up tachyons_pt2_3ym tachyons_silver_3lN style__focus_12t style_large_5KY" id="q" name="q" placeholder="Search..." data-reactid="12"/></span><button class="style_button_23U tachyons_br2_rOa tachyons_dim_1n1 tachyons_flex-auto_1Fv tachyons_ma0_2xh tachyons_pointer_25K style_light_3PJ tachyons_bg-white_WF0 tachyons_bn_3W7 tachyons_f4_1Zt tachyons_pa1_3f3 theme_color-blue_2lo" type="submit" data-reactid="13"><svg fill="currentColor" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" viewBox="0 0 40 40" style="vertical-align:middle;color:inherit;" data-reactid="14"><g data-reactid="15"><path d="m27.2 18.6q0-4.2-2.9-7.1t-7.1-2.9-7 2.9-3 7.1 2.9 7 7.1 3 7.1-3 2.9-7z m11.4 18.5q0 1.2-0.8 2.1t-2 0.8q-1.2 0-2-0.8l-7.7-7.7q-4 2.8-8.9 2.8-3.2 0-6.1-1.3t-5-3.3-3.4-5-1.2-6.1 1.2-6.1 3.4-5.1 5-3.3 6.1-1.2 6.1 1.2 5 3.3 3.4 5.1 1.2 6.1q0 4.9-2.7 8.9l7.6 7.6q0.8 0.9 0.8 2z" data-reactid="16"></path></g></svg></button></form></div></header></div><main class="style_main_GV3 tachyons_bg-white_WF0 tachyons_center_iwf tachyons_mt5_2gs tachyons_mw-100_ge3 tachyons_mw7-m_FjT tachyons_mw8-l_KcP tachyons_ph3_Qxl tachyons_ph4-l_pNz tachyons_pb4_38s tachyons_pt3_1QC" data-reactid="17"><!-- react-empty: 18 --><div class="markdown" data-reactid="19"><!-- react-empty: 20 --><h1 data-reactid="21">Learning Through Active Exploration</h1><div data-reactid="22"><p>When I first arrived at Silicon Valley for my internship, the entire environment
looked new. It took me several weeks to get familiar with my new neighborhood.
Interestingly, even with advanced GPS apps on my phone, the most effective way
to learn a new environment is to walk on the street, memorizing landmarks, and
making different turns at intersections. The GPS app could give me smart
directions, but I could not really learn the world just by staring at the map.
Knowledge comes from practice, and we always learn something through active
exploration. Nevertheless, most artificial intelligence techniques adopt data
intensive machine learning approaches. Algorithms are trained to find patterns
by observing massive amounts of data passively, usually without generating any
actions.</p>
<p>At Numenta, we are working on a next-generation machine intelligence algorithm
that learns complex patterns through active exploration. The stream of sensory
inputs is actively generated by execution of a series of motor commands. We call
this new learning paradigm sensorimotor learning and prediction, or SLAP. To
understand how the algorithm works, let us first think of how our brain solves
the same problem.</p>
<p>We know that all our remarkable cognitive abilities, object recognition, scene
interpretation, reasoning and prediction, starts from data streams collected by
our “sensors”, such as retina at the back of eyes, tactile sensors under the
skin and auditory sensors in the cochlea. Believe it or not, most of the inputs
to the sensors are actually generated by ourselves, rather than by changes in
the external world. Our eyes are constantly moving; our touch senses mostly
arise from our own body movement, and the speech we generated is also picked up
by our auditory nerves. After we learn a new environment, we are rarely
surprised by the consequences of our own actions. I can predict exactly what I
will see after each turn on my way to work now. This prediction is based on my
current sensory input and the motor command I am going to execute. Moreover,
despite dramatic changes of input to my sensors, my internal perception is
stable. These two aspects reflect two component of the algorithm. We call the
prediction step “sensorimotor inference”, and the process of building stable
representations as “temporal pooling”.</p>
<p><img src="/assets/img/pages/blog/2014-10-07/1.png" alt="Neuroscience Image"></p>
<p>Jeff Hawkins described the basic ideas on the
<a href="http://numenta.org/lists/">NuPIC mailing list</a>. During my internship I
implemented and worked on several SLAP experiments using synthetic
datasets. In one experiment, we trained the SLAP algorithm to recognize a large
number of synthetic images composed of “squares”. Each square is painted with
different color and different images share the same set of colors. Two example
images are shown below. A white diamond represents the portion of the image that
lies on the fovea, and a black arrow represents the proposed motor command.</p>
<p><img src="/assets/img/pages/blog/2014-10-07/2.png" alt="Neuroscience Image"></p>
<p>The algorithm is allowed to explore each image through simulated eye-movements.
At each step the image under the fovea and the motor command is fed to the
algorithm. The first layer of the network learns to make predictions of the next
sensory input. The algorithm also utilizes a reasonable assumption that if two
things are close to each other in time (temporal proximity), they tend to
originate from the same underlying cause, and thus should be grouped together.
Neurons of the second layer “pool” over many neurons in the first layer, and
form a stable representation that is unique to each pattern. During learning,
stable representations will emerge despite changes to the input in the first
layer. These stable representations indicate recognition of the larger image.</p>
<p>The figure below shows example output from a trained system while the eyes are
moving around two different images (10 iterations for each image). At each step,
the sensory input is changing drastically. However, the overall output is a
stable and unique representation for each image.</p>
<p><img src="/assets/img/pages/blog/2014-10-07/3.png" alt="Neuroscience Image"></p>
<p>This simple example illustrates a fundamental mechanism our brain uses to create
stable representations from a changing world. The same mechanism can also be
used for a large variety of problems where the sensor data is actively generated
by the system, such as robot learning, vehicle control, and complex pattern
recognition/detection problems.</p>
<p>Note: Some of my implementation code is now available in the
<a href="https://github.com/numenta/nupic.research">nupic.research github repository</a>.</p>
</div></div></main><div class="style_footbar_D75 tachyons_w-100_1xC" data-reactid="23"><footer class="style_footer_XFN tachyons_br--bottom_1e0 tachyons_bg-white_WF0 tachyons_br2_rOa tachyons_center_iwf tachyons_f6_14U tachyons_mw-100_ge3 tachyons_mw7-m_FjT tachyons_mw8-l_KcP tachyons_pv3_1Zi tachyons_silver_3lN tachyons_tc_3ab" data-reactid="24"><span data-reactid="25"><!-- react-text: 26 -->© <!-- /react-text --><!-- react-text: 27 -->2016<!-- /react-text --><!-- react-text: 28 --> <!-- /react-text --><!-- react-text: 29 --> <!-- /react-text --><a class="style_textlink_206 tachyons_dim_1n1 tachyons_link_2zV tachyons_underline-hover_1xv theme_color-blue-dark_2u0 style__visited_BRb" href="/numenta-web/" data-reactid="30">Numenta</a></span><span class="style_mark_2c_ tachyons_dib_1gH tachyons_h1_Kza tachyons_mh3_2gH tachyons_relative_1fO tachyons_w1_1jO" data-reactid="31"><img alt="Numenta Logo Mark" class="style_image_Kv4 tachyons_br2_rOa" src="/numenta-web/87b23beb8a4b7dea7d88099bfb28d182.svg" title="Numenta" data-reactid="32"/></span><span data-reactid="33"><a class="style_textlink_206 tachyons_dim_1n1 tachyons_link_2zV tachyons_underline-hover_1xv theme_color-blue-dark_2u0 style__visited_BRb" href="/numenta-web/legal/privacy/" data-reactid="34">Privacy</a><span class="style_spacer_1h0 tachyons_mh1_-Z3 tachyons_silver_3lN" data-reactid="35">•</span><a class="style_textlink_206 tachyons_dim_1n1 tachyons_link_2zV tachyons_underline-hover_1xv theme_color-blue-dark_2u0 style__visited_BRb" href="/numenta-web/legal/terms/" data-reactid="36">Terms</a><span class="style_spacer_1h0 tachyons_mh1_-Z3 tachyons_silver_3lN" data-reactid="37">•</span><a class="style_textlink_206 tachyons_dim_1n1 tachyons_link_2zV tachyons_underline-hover_1xv theme_color-blue-dark_2u0 style__visited_BRb" href="https://github.com/numenta/numenta-web" data-reactid="38">Source</a><span class="style_spacer_1h0 tachyons_mh1_-Z3 tachyons_silver_3lN" data-reactid="39">•</span><a class="style_textlink_206 tachyons_dim_1n1 tachyons_link_2zV tachyons_underline-hover_1xv theme_color-blue-dark_2u0 style__visited_BRb" href="/numenta-web/sitemap/" data-reactid="40">Sitemap</a></span></footer></div></div></div><script src="/numenta-web/bundle.js?v=0.2.25"></script></body></html>